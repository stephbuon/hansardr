hansard_1840 <- hansard_1840 %>%
left_join(debate_metadata_1840)
hansard_1850 <- hansard_1850 %>%
left_join(debate_metadata_1850)
hansard_1860 <- hansard_1860 %>%
left_join(debate_metadata_1860)
crimea_data <- hansard_1840 %>%
bind_rows(hansard_1850) %>%
bind_rows(hansard_1860) %>%
#mutate(year = year(speechdate)) %>%
#filter(!is.na(year)) %>%
filter(speechdate >= 1-1-1848) %>%
filter(speechdate <= 1-1-1860)
crimea_data_periodized <- crimea_data %>%
mutate(speechdate= as.Date(speechdate)) %>%
mutate(period = case_when(speechdate < as.Date("1853-10-05") ~ "pre-Crimea",
speechdate >= as.Date("1853-10-05") & speechdate < as.Date("1856-3-30") ~ "Crimean War",
speechdate > as.Date("1856-3-30") ~ "post-Crimea", TRUE ~ "other")) %>%
select(-sentence_id, -decade, -debate)
tokenized_crimea <- crimea_data_periodized %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
filter(!str_detect(word, "[:digit:]"))
crimea_count <- tokenized_crimea %>%
count(period, word, sort = TRUE) %>%
filter(n > 20)
total_crimea_count <- crimea_count %>%
group_by(period) %>%
summarize(total = sum(n))
crimea_words <- left_join(total_crimea_count, crimea_count)
crimea_words %>%
bind_tf_idf(word, period, n) %>%
group_by(period) %>%
slice_max(tf_idf, n = 15) %>%
ggplot(aes(tf_idf,
reorder(word, tf_idf),
fill = period)) +
geom_col(show.legend = FALSE) +
facet_wrap(~period, ncol = 2, scales = "free") +
labs(x = "TF-IDF", y = "Word")
library(lubridate)
data("hansard_1860")
data("debate_metadata_1840")
data("debate_metadata_1850")
data("debate_metadata_1860")
hansard_1840 <- hansard_1840 %>%
left_join(debate_metadata_1840)
hansard_1850 <- hansard_1850 %>%
left_join(debate_metadata_1850)
hansard_1860 <- hansard_1860 %>%
left_join(debate_metadata_1860)
crimea_data <- hansard_1840 %>%
bind_rows(hansard_1850) %>%
bind_rows(hansard_1860) %>%
#mutate(year = year(speechdate)) %>%
#filter(!is.na(year)) %>%
filter(speechdate >= as.Date("1848-1-1"),
speechdate <= as.Date("1860-1-1"))
crimea_data_periodized <- crimea_data %>%
mutate(speechdate= as.Date(speechdate)) %>%
mutate(period = case_when(speechdate < as.Date("1853-10-05") ~ "pre-Crimea",
speechdate >= as.Date("1853-10-05") & speechdate < as.Date("1856-3-30") ~ "Crimean War",
speechdate > as.Date("1856-3-30") ~ "post-Crimea", TRUE ~ "other")) %>%
select(-sentence_id, -decade, -debate)
tokenized_crimea <- crimea_data_periodized %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
filter(!str_detect(word, "[:digit:]"))
crimea_count <- tokenized_crimea %>%
count(period, word, sort = TRUE) %>%
filter(n > 20)
total_crimea_count <- crimea_count %>%
group_by(period) %>%
summarize(total = sum(n))
crimea_words <- left_join(total_crimea_count, crimea_count)
crimea_words %>%
bind_tf_idf(word, period, n) %>%
group_by(period) %>%
slice_max(tf_idf, n = 15) %>%
ggplot(aes(tf_idf,
reorder(word, tf_idf),
fill = period)) +
geom_col(show.legend = FALSE) +
facet_wrap(~period, ncol = 2, scales = "free") +
labs(x = "TF-IDF", y = "Word")
require(devtools)
install_github("stephbuon/dhmeasures")
library(dhmeasures)
library(hansardr)
library(tidyverse)
library(tidytext)
data("hansard_1830")
data("hansard_1860")
hansard_1830 <- hansard_1830 %>%
unnest_tokens(word, text) %>%
count(word, sort = TRUE) %>%
anti_join(stop_words) %>%
filter(!str_detect(word, "[:digit:]")) %>%
mutate(corpus = 1830)
hansard_1860 <- hansard_1860 %>%
unnest_tokens(word, text) %>%
count(word, sort = TRUE) %>%
anti_join(stop_words) %>%
filter(!str_detect(word, "[:digit:]")) %>%
mutate(corpus = 1860)
hansard <- bind_rows(hansard_1830, hansard_1860)
jsd_out <- jsd(hansard, group = "corpus")
knitr::opts_chunk$set(echo = TRUE)
library(dhmeasures)
hansard <- bind_rows(hansard_1830, hansard_1860)
jsd_out = jsd(hansard, group = "corpus")
library(dhmeasures)
hansard <- bind_rows(hansard_1830, hansard_1860)
View(jsd_out)
head(hansard_jsd)
hansard_jsd <- jsd(hansard, group = "corpus")
head(hansard_jsd)
View(hansard_jsd)
decades <- c(1820, 1860)
for(decade in decades) {
top_jsd <- hansard_jsd %>%
top_n(20, wt = X1860_1830)
ggplot(top_jsd,
aes(x = reorder(word, X1860_1830),
y = X1860_1830,
fill = X1860_1830)) +
geom_col() +
coord_flip() +
labs(x = "word",
title = paste("Words Most Likely to Appear in", decade)) }
decades <- c(1820, 1860)
for(decade in decades) {
top_jsd <- hansard_jsd %>%
top_n(20, wt = X1860_1830)
ggplot(top_jsd,
aes(x = reorder(word, X1860_1830),
y = X1860_1830,
fill = X1860_1830)) +
geom_col() +
coord_flip() +
labs(x = "word",
title = paste("Words Most Likely to Appear in", decade)) }
#, echo = F}
decades <- c(1820, 1860)
for(decade in decades) {
top_jsd <- hansard_jsd %>%
top_n(20, wt = X1860_1830)
ggplot(top_jsd,
aes(x = reorder(word, X1860_1830),
y = X1860_1830,
fill = X1860_1830)) +
geom_col() +
coord_flip() +
labs(x = "word",
title = paste("Words Most Likely to Appear in", decade)) }
#, echo = F}
top_jsd <- hansard_jsd %>%
top_n(20, wt = X1860_1830)
ggplot(top_jsd,
aes(x = reorder(word, X1860_1830),
y = X1860_1830,
fill = X1860_1830)) +
geom_col() +
coord_flip() +
labs(x = "word",
title = paste("Words Most Likely to Appear in", decade))
knitr::opts_chunk$set(echo = TRUE)
data("hansard_1830")
data("hansard_1860")
hansard_1830 <- hansard_1830 %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
filter(!str_detect(word, "[:digit:]")) %>%
mutate(period = 1830)
knitr::opts_chunk$set(echo = TRUE)
library(hansardr)
library(tidyverse)
library(tidytext)
data("stop_words")
data("hansard_1830")
data("hansard_1860")
hansard_1830 <- hansard_1830 %>%
mutate(decade = 1830)
hansard_1860 <- hansard_1860 %>%
mutate(decade = 1860)
hansard_data <- bind_rows(hansard_1830, hansard_1860)
head(hansard_data)
tokenized_hansard_data <- hansard_data %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
filter(!str_detect(word, "[:digit:]"))
head(tokenized_hansard_data)
hansard_count <- tokenized_hansard_data %>%
count(decade, word, sort = TRUE) %>%
filter(n > 1)
total_count <- hansard_count %>%
group_by(decade) %>%
summarize(total = sum(n))
head(hansard_count)
head(total_count)
hansard_words <- left_join(hansard_count, total_count)
hansard_tf_idf <- hansard_words %>%
bind_tf_idf(word, decade, n)
head(hansard_tf_idf)
hansard_tf_idf <- hansard_tf_idf %>%
group_by(decade) %>%
slice_max(tf_idf, n = 15)
ggplot(hansard_tf_idf,
aes(tf_idf,
reorder(word, tf_idf),
fill = decade)) +
geom_col(show.legend = FALSE) +
facet_wrap(~decade, ncol = 2, scales = "free") +
labs(x = "TF-IDF", y = "Word")
data("hansard_1840")
data("hansard_1850")
hansard_1840 <- hansard_1840 %>%
mutate(decade = 1840)
hansard_1850 <- hansard_1850 %>%
mutate(decade = 1850)
hansard_data <- hansard_data %>%
bind_rows(hansard_1840) %>%
bind_rows(hansard_1850)
decades <- c(1830, 1840, 1850, 1860)
hansard_count <- data.frame()
for(d in decades) {
hansard_decade <- hansard_data %>%
filter(decade == d)
tokenized <- hansard_decade %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
filter(!str_detect(word, "[:digit:]"))
hansard_decade_count <- tokenized %>%
count(decade, word, sort = TRUE) %>%
filter(n > 20)
hansard_count <- bind_rows(hansard_count, hansard_decade_count) }
head(hansard_count)
total_count <- hansard_count %>%
group_by(decade) %>%
summarize(total = sum(n))
head(total_count)
hansard_words <- left_join(hansard_count, total_count)
hansard_words %>%
bind_tf_idf(word, decade, n) %>%
group_by(decade) %>%
slice_max(tf_idf, n = 15) %>%
ggplot(aes(tf_idf,
reorder(word, tf_idf),
fill = decade)) +
geom_col(show.legend = FALSE) +
facet_wrap(~decade, ncol = 2, scales = "free") +
labs(x = "TF-IDF", y = "Word")
library(lubridate)
data("hansard_1860")
data("debate_metadata_1840")
data("debate_metadata_1850")
data("debate_metadata_1860")
hansard_1840 <- hansard_1840 %>%
left_join(debate_metadata_1840)
hansard_1850 <- hansard_1850 %>%
left_join(debate_metadata_1850)
hansard_1860 <- hansard_1860 %>%
left_join(debate_metadata_1860)
crimea_data <- hansard_1840 %>%
bind_rows(hansard_1850) %>%
bind_rows(hansard_1860) %>%
filter(speechdate >= as.Date("1848-1-1"),
speechdate <= as.Date("1860-1-1"))
crimea_data_periodized <- crimea_data %>%
mutate(speechdate= as.Date(speechdate)) %>%
mutate(period = case_when(speechdate < as.Date("1853-10-05") ~ "pre-Crimea",
speechdate >= as.Date("1853-10-05") & speechdate < as.Date("1856-3-30") ~ "Crimean War",
speechdate > as.Date("1856-3-30") ~ "post-Crimea", TRUE ~ "other")) %>%
select(-sentence_id, -decade, -debate)
tokenized_crimea <- crimea_data_periodized %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
filter(!str_detect(word, "[:digit:]"))
crimea_count <- tokenized_crimea %>%
count(period, word, sort = TRUE) %>%
filter(n > 20)
total_crimea_count <- crimea_count %>%
group_by(period) %>%
summarize(total = sum(n))
crimea_words <- left_join(total_crimea_count, crimea_count)
crimea_words %>%
bind_tf_idf(word, period, n) %>%
group_by(period) %>%
slice_max(tf_idf, n = 15) %>%
ggplot(aes(tf_idf,
reorder(word, tf_idf),
fill = period)) +
geom_col(show.legend = FALSE) +
facet_wrap(~period, ncol = 2, scales = "free") +
labs(x = "TF-IDF", y = "Word")
keyword_search <- crimea_data_periodized %>%
filter(str_detect(text, "outdoor relief"))
keyword_count <- keyword_search %>%
mutate(count = str_count(text, "outdoor relief")) %>%
mutate(yearmonth = floor_date(speechdate, "month")) %>%
group_by(yearmonth) %>%
summarize(count = sum(count))
ggplot(keyword_count, aes(x = yearmonth, y = count)) +
geom_col() +
ggtitle("How Many Times Was Outdoor Relief Mentioned?")
library(quanteda)
top_month <- keyword_search %>%
filter(speechdate >= "1850-02-01")  %>%
filter(speechdate <= "1850-02-28")
top_month_kwic <- corpus(top_month, text_field = "text")
kwic(tokens(top_month_kwic),
"outdoor",
window = 5,
case_insensitive = TRUE)
data("speaker_metadata_1860")
speaker_metadata_1860 <- speaker_metadata_1860 %>%
select(sentence_id, suggested_speaker)
head(speaker_metadata_1860)
hansard_1860 <- left_join(hansard_1860, speaker_metadata_1860)
speaker_count <- hansard_1860 %>%
filter(suggested_speaker %in% c("william_gladstone_3104", "benjamin_disraeli_3523")) %>%
unnest_tokens(word, text)  %>%
anti_join(stop_words) %>%
filter(!str_detect(word, "[:digit:]"))  %>%
count(suggested_speaker, word, sort = TRUE) %>%
filter(n > 1)
total_count <- speaker_count %>%
group_by(suggested_speaker) %>%
summarize(total = sum(n))
hansard_words <- left_join(speaker_count, total_count)
hansard_tf_idf <- hansard_words %>%
bind_tf_idf(word, suggested_speaker, n)
hansard_tf_idf <- hansard_tf_idf %>%
group_by(suggested_speaker) %>%
slice_max(tf_idf, n = 15)
ggplot(hansard_tf_idf,
aes(tf_idf, reorder(word, tf_idf), fill = suggested_speaker)) +
geom_col(show.legend = FALSE) +
facet_wrap(~suggested_speaker, ncol = 2, scales = "free") +
labs(x = "TF-IDF", y = "Word")
data("hansard_1830")
data("hansard_1860")
hansard_1830 <- hansard_1830 %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
filter(!str_detect(word, "[:digit:]")) %>%
mutate(period = 1830)
hansard_1860 <- hansard_1860 %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
filter(!str_detect(word, "[:digit:]")) %>%
mutate(period = 1860)
data("hansard_1830")
data("hansard_1860")
hansard_1830 <- hansard_1830 %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
filter(!str_detect(word, "[:digit:]")) %>%
mutate(period = 1830)
hansard_1860 <- hansard_1860 %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
filter(!str_detect(word, "[:digit:]")) %>%
mutate(period = 1860)
library(dhmeasures)
hansard <- bind_rows(hansard_1830, hansard_1860)
hansard_jsd <- jsd(hansard, group = "period")
data("hansard_1830")
data("hansard_1860")
hansard_1830 <- hansard_1830 %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
filter(!str_detect(word, "[:digit:]")) %>%
mutate(corpus = 1830)
hansard_1860 <- hansard_1860 %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
filter(!str_detect(word, "[:digit:]")) %>%
mutate(corpus = 1860)
library(dhmeasures)
hansard <- bind_rows(hansard_1830, hansard_1860)
hansard_jsd <- jsd(hansard, group = "corpus")
require(devtools)
install_github("stephbuon/dhmeasures")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(hansardr)
data("hansard_1870")
hansard_woman_1870 <- hansard_1870 %>%
filter(str_detect(text, regex("woman|women", ignore_case = T)))
library(tidyverse)
library(hansardr)
data("hansard_1870")
hansard_woman_1870 <- hansard_1870 %>%
filter(str_detect(text, regex("woman|women", ignore_case = T)))
data("debate_metadata_1870")
debate_metadata_1870 <- debate_metadata_1870 %>%
select(sentence_id, speechdate)
hansard_woman_1870 <- left_join(hansard_woman_1870, debate_metadata_1870)
filtered_hansard_woman_1870 <- hansard_woman_1870 %>%
select(text)
filtered_hansard_woman_1870 <- hansard_woman_1870 %>%
select(text)
adjective_noun_pairs <- extract_adj_noun_pairs(filtered_hansard_woman_1870$text)
library(posextractr)
posextract_install()
require(devtools)
install_github("stephbuon/posextractr")
library("posextractr")
posextract_install()
library("posextractr")
posextract_install()
library("posextractr")
posextract_install()
posextract_install()
require(devtools)
install_github("stephbuon/posextractr")
library("posextractr")
posextract_install()
Y
require(devtools)
install_github("stephbuon/posextractr")
library("posextractr")
posextract_install()
require(devtools)
install_github("stephbuon/posextractr")
require(devtools)
install_github("stephbuon/posextractr")
library("posextractr")
posextract_install()
require(devtools)
install_github("stephbuon/posextractr")
library("posextractr")
posextract_install()
posextract_install()
Y
posextract_initialize()
knitr::opts_chunk$set(echo = TRUE)
library(posextractr)
posextract_install()
Y
posextract_initialize()
library(tidyverse)
library(hansardr)
data("hansard_1870")
hansard_woman_1870 <- hansard_1870 %>%
filter(str_detect(text, regex("woman|women", ignore_case = T)))
head(hansard_woman_1870)
data("debate_metadata_1870")
debate_metadata_1870 <- debate_metadata_1870 %>%
select(sentence_id, speechdate)
hansard_woman_1870 <- left_join(hansard_woman_1870, debate_metadata_1870)
head(hansard_woman_1870)
filtered_hansard_woman_1870 <- hansard_woman_1870 %>%
select(text)
adjective_noun_pairs <- extract_adj_noun_pairs(filtered_hansard_woman_1870$text, lemmatize = T)
library(posextractr)
adjective_noun_pairs <- extract_adj_noun_pairs(filtered_hansard_woman_1870$text, lemmatize = T)
require(devtools)
install_github("stephbuon/posextractr")
require(devtools)
install_github("stephbuon/posextractr")
library("posextractr")
posextract_install()
Y
posextract_initialize()
adjective_noun_pairs <- extract_adj_noun_pairs(filtered_hansard_woman_1870$text, lemmatize = T)
adjective_noun_pairs <- extract_adj_noun_pairs(filtered_hansard_woman_1870$text, lemmatize = T)
library(posextractr)
triples <- extract_triples(hansard_woman_1870$text,
combine_adj = F,
lemmatize = T,
add_aux = F)
knitr::opts_chunk$set(echo = TRUE)
library(posextractr)
posextract_install()
posextract_initialize()
library(tidyverse)
library(hansardr)
data("hansard_1870")
hansard_woman_1870 <- hansard_1870 %>%
filter(str_detect(text, regex("woman|women", ignore_case = T)))
head(hansard_woman_1870)
data("debate_metadata_1870")
debate_metadata_1870 <- debate_metadata_1870 %>%
select(sentence_id, speechdate)
hansard_woman_1870 <- left_join(hansard_woman_1870, debate_metadata_1870)
head(hansard_woman_1870)
filtered_hansard_woman_1870 <- hansard_woman_1870 %>%
select(text)
adjective_noun_pairs <- extract_adj_noun_pairs(filtered_hansard_woman_1870$text, lemmatize = T)
library(posextractr)
adjective_noun_pairs <- extract_adj_noun_pairs(filtered_hansard_woman_1870$text, lemmatize = T)
test <- hansard_woman_1870 %>%
sample_n(1000)
triples <- extract_triples(test$text,
combine_adj = F,
lemmatize = T,
add_aux = F)
library(posextractr)
posextract_install()
Y
posextract_initialize()
adjective_noun_pairs <- extract_adj_noun_pairs(filtered_hansard_woman_1870$text, lemmatize = T)
setwd("~/projects/hansardr")
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
